{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "import re\n",
    "import os\n",
    "import codecs\n",
    "from sklearn import feature_extraction\n",
    "import mpld3\n",
    "import requests\n",
    "from requests_oauthlib import OAuth1\n",
    "from os.path import expanduser\n",
    "import time\n",
    "home = expanduser(\"~\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Importing our Config\n",
    "\n",
    "# import cnfg\n",
    "# config = cnfg.load(home + \"/.twitter_config\")\n",
    "\n",
    "# oauth = OAuth1(config[\"consumer_key\"],\n",
    "#                config[\"consumer_secret\"],\n",
    "#                config[\"access_token\"],\n",
    "#                config[\"access_token_secret\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def get_all_tweets(query, batch_count, iter_count):\n",
    "#     parameters = {\"q\": query, \"count\": batch_count}\n",
    "#     base_url = \"https://api.twitter.com/1.1/search/tweets.json\"\n",
    "#     response = requests.get(base_url,\n",
    "#                         params = parameters,\n",
    "#                         auth=oauth)\n",
    "#     tweets = response.json()['statuses']\n",
    "#     i = 0\n",
    "#     while i < iter_count - 1:\n",
    "#         try:\n",
    "#             response = requests.get(base_url + response.json()['search_metadata']['next_results'],\n",
    "#                                 auth=oauth)\n",
    "#             tweets.extend(response.json())\n",
    "#             print (i)\n",
    "#             print (len(tweets))\n",
    "#             i += 1\n",
    "#         except:\n",
    "#             return tweets\n",
    "#             time.sleep(900)\n",
    "#     return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tweets = get_all_tweets('budlight', 100, 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will use the tweepy package to authenticate and connect to the twitter API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import tweepy\n",
    "\n",
    "# auth = tweepy.OAuthHandler(config[\"consumer_key\"],\n",
    "#                            config[\"consumer_secret\"])\n",
    "# auth.set_access_token(config[\"access_token\"],\n",
    "#                       config[\"access_token_secret\"])\n",
    "\n",
    "# api=tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# query = 'budlight'\n",
    "# max_tweets=2000\n",
    "# attempts = 50\n",
    "# attempt = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# retry = True\n",
    "# while retry:\n",
    "#     try:\n",
    "#         retry = False\n",
    "#         tweets = tweepy.Cursor(api.search,q=query,lang=\"en\").items(max_tweets)\n",
    "#         for tweet in tweets:\n",
    "#             results.append(tweet)\n",
    "#     except tweepy.TweepError:\n",
    "#         retry = True\n",
    "#         time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now we have about 5000 tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pd.to_pickle(results,'results_budlight.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results = pd.read_pickle('results_budlight.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nathanbackblaze/anaconda/lib/python2.7/site-packages/IPython/core/formatters.py:92: DeprecationWarning: DisplayFormatter._ipython_display_formatter_default is deprecated: use @default decorator instead.\n",
      "  def _ipython_display_formatter_default(self):\n",
      "/Users/nathanbackblaze/anaconda/lib/python2.7/site-packages/IPython/core/formatters.py:98: DeprecationWarning: DisplayFormatter._formatters_default is deprecated: use @default decorator instead.\n",
      "  def _formatters_default(self):\n",
      "/Users/nathanbackblaze/anaconda/lib/python2.7/site-packages/IPython/core/formatters.py:677: DeprecationWarning: PlainTextFormatter._deferred_printers_default is deprecated: use @default decorator instead.\n",
      "  def _deferred_printers_default(self):\n",
      "/Users/nathanbackblaze/anaconda/lib/python2.7/site-packages/IPython/core/formatters.py:669: DeprecationWarning: PlainTextFormatter._singleton_printers_default is deprecated: use @default decorator instead.\n",
      "  def _singleton_printers_default(self):\n",
      "/Users/nathanbackblaze/anaconda/lib/python2.7/site-packages/IPython/core/formatters.py:672: DeprecationWarning: PlainTextFormatter._type_printers_default is deprecated: use @default decorator instead.\n",
      "  def _type_printers_default(self):\n",
      "/Users/nathanbackblaze/anaconda/lib/python2.7/site-packages/IPython/core/formatters.py:669: DeprecationWarning: PlainTextFormatter._singleton_printers_default is deprecated: use @default decorator instead.\n",
      "  def _singleton_printers_default(self):\n",
      "/Users/nathanbackblaze/anaconda/lib/python2.7/site-packages/IPython/core/formatters.py:672: DeprecationWarning: PlainTextFormatter._type_printers_default is deprecated: use @default decorator instead.\n",
      "  def _type_printers_default(self):\n",
      "/Users/nathanbackblaze/anaconda/lib/python2.7/site-packages/IPython/core/formatters.py:677: DeprecationWarning: PlainTextFormatter._deferred_printers_default is deprecated: use @default decorator instead.\n",
      "  def _deferred_printers_default(self):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5155"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'RT @budlight: The new #BudLightParty look is now all across America, even under your seat. https://t.co/hqBVP48DWC'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[115].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The twitter api returns an array of hastags, let's organize this into a set of all the hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_hashtags(results):\n",
    "    hashtags_list = []\n",
    "    for result in results:\n",
    "        hashtags = result.entities['hashtags']\n",
    "        for hashtag in hashtags:\n",
    "            hashtags_list.append(hashtag['text'].lower())\n",
    "    return hashtags_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hashtags_set = get_hashtags(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hashtags_set = list(set(hashtags_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(hashtags_set, columns=['hashtags'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm tipsy in the after math of my two hour long stream, from ONE 8oz can of budlight strawberita. The hell?\n",
      "Captain Budlight in this beezy https://t.co/1tKcuZ8Cvg\n",
      "If he is @JohnCollins it will be the last cool one in a long time. So, I'd hope he'd choose something better than a Budlight. \n",
      "@Itsjoeco\n",
      "RT @gabivers: Budlight apple, I could marry you\n",
      "It's Copa Day! Mexico vs. Venezuela starting at 5pm! We have $3 beers of Montejo, Budlight and Coors! ⚽󾦇... https://t.co/FMQlqU4Ag0\n"
     ]
    }
   ],
   "source": [
    "for result in results[:5]:\n",
    "    print result.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_hash_corpus(row):\n",
    "    row = row.lower()\n",
    "    corpus = u''\n",
    "    for result in results:\n",
    "        hashtag_list = []\n",
    "        for sub_tag in result.entities['hashtags']:\n",
    "            hashtag_list.append(sub_tag['text'].lower())\n",
    "        if row in hashtag_list:\n",
    "            corpus += result.text\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['corpus'] = [get_hash_corpus(row) for row in df['hashtags']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hashtags</th>\n",
       "      <th>corpus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>740</th>\n",
       "      <td>watchparty</td>\n",
       "      <td>Join us tonight for a #Cubs #watchparty with #...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>741</th>\n",
       "      <td>petesonpike</td>\n",
       "      <td>Starting vacation #1 off right. #petesonpike #...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>742</th>\n",
       "      <td>ProGayAgenda</td>\n",
       "      <td>Look at all the bigot #MORONS turning against ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>743</th>\n",
       "      <td>coldbrew</td>\n",
       "      <td>It's a cold bud light kind of night. #budlight...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>744</th>\n",
       "      <td>southcarolina</td>\n",
       "      <td>That's how we do a #budlight #yall #southcarol...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          hashtags                                             corpus\n",
       "740     watchparty  Join us tonight for a #Cubs #watchparty with #...\n",
       "741    petesonpike  Starting vacation #1 off right. #petesonpike #...\n",
       "742   ProGayAgenda  Look at all the bigot #MORONS turning against ...\n",
       "743       coldbrew  It's a cold bud light kind of night. #budlight...\n",
       "744  southcarolina  That's how we do a #budlight #yall #southcarol..."
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-f83a666c8d2e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'budlight_hashed.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "pd.to_pickle(df,'budlight_hashed.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_pickle('budlight_hashed.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nathanbackblaze/anaconda/lib/python2.7/site-packages/IPython/core/formatters.py:92: DeprecationWarning: DisplayFormatter._ipython_display_formatter_default is deprecated: use @default decorator instead.\n",
      "  def _ipython_display_formatter_default(self):\n",
      "/Users/nathanbackblaze/anaconda/lib/python2.7/site-packages/IPython/core/formatters.py:98: DeprecationWarning: DisplayFormatter._formatters_default is deprecated: use @default decorator instead.\n",
      "  def _formatters_default(self):\n",
      "/Users/nathanbackblaze/anaconda/lib/python2.7/site-packages/IPython/core/formatters.py:677: DeprecationWarning: PlainTextFormatter._deferred_printers_default is deprecated: use @default decorator instead.\n",
      "  def _deferred_printers_default(self):\n",
      "/Users/nathanbackblaze/anaconda/lib/python2.7/site-packages/IPython/core/formatters.py:669: DeprecationWarning: PlainTextFormatter._singleton_printers_default is deprecated: use @default decorator instead.\n",
      "  def _singleton_printers_default(self):\n",
      "/Users/nathanbackblaze/anaconda/lib/python2.7/site-packages/IPython/core/formatters.py:672: DeprecationWarning: PlainTextFormatter._type_printers_default is deprecated: use @default decorator instead.\n",
      "  def _type_printers_default(self):\n",
      "/Users/nathanbackblaze/anaconda/lib/python2.7/site-packages/IPython/core/formatters.py:669: DeprecationWarning: PlainTextFormatter._singleton_printers_default is deprecated: use @default decorator instead.\n",
      "  def _singleton_printers_default(self):\n",
      "/Users/nathanbackblaze/anaconda/lib/python2.7/site-packages/IPython/core/formatters.py:672: DeprecationWarning: PlainTextFormatter._type_printers_default is deprecated: use @default decorator instead.\n",
      "  def _type_printers_default(self):\n",
      "/Users/nathanbackblaze/anaconda/lib/python2.7/site-packages/IPython/core/formatters.py:677: DeprecationWarning: PlainTextFormatter._deferred_printers_default is deprecated: use @default decorator instead.\n",
      "  def _deferred_printers_default(self):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hashtags</th>\n",
       "      <th>corpus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>740</th>\n",
       "      <td>watchparty</td>\n",
       "      <td>Join us tonight for a #Cubs #watchparty with #...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>741</th>\n",
       "      <td>petesonpike</td>\n",
       "      <td>Starting vacation #1 off right. #petesonpike #...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>742</th>\n",
       "      <td>ProGayAgenda</td>\n",
       "      <td>Look at all the bigot #MORONS turning against ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>743</th>\n",
       "      <td>coldbrew</td>\n",
       "      <td>It's a cold bud light kind of night. #budlight...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>744</th>\n",
       "      <td>southcarolina</td>\n",
       "      <td>That's how we do a #budlight #yall #southcarol...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          hashtags                                             corpus\n",
       "740     watchparty  Join us tonight for a #Cubs #watchparty with #...\n",
       "741    petesonpike  Starting vacation #1 off right. #petesonpike #...\n",
       "742   ProGayAgenda  Look at all the bigot #MORONS turning against ...\n",
       "743       coldbrew  It's a cold bud light kind of night. #budlight...\n",
       "744  southcarolina  That's how we do a #budlight #yall #southcarol..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a data frame where the 'hashtags' column has all the hastags mentioned in the budlight twitter query, and 'corpus' has all the text from all the tweets containing the respective hashtag.\n",
    "\n",
    "This will essentially tell us which text is used in relation to each hashtag, we can now use tfidf and Latent Semantic Analysis to find common topics\n",
    "\n",
    "For this portion I choose to use the Textacy library (which is built on spaCy) for a few reasons:\n",
    "- Textacy/spaCy run in Cython, so it is high-performing.  This import because the application will eventually be user-facing\n",
    "- Textacy/spaCy can handle odd terms such as emojis\n",
    "- Textacy has an implementation for TFIDF (among other features), which might be import for filtering out unimport words in the terms matrix\n",
    "- Textacy has Latent Semantic Analysis built it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nathanbackblaze/anaconda/lib/python2.7/site-packages/numpy/lib/utils.py:99: DeprecationWarning: `scipy.sparse.sparsetools` is deprecated!\n",
      "scipy.sparse.sparsetools is a private module for scipy.sparse, and should not be used.\n",
      "  warnings.warn(depdoc, DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import textacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "docs = df['corpus']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpus = textacy.TextCorpus.from_texts('en',docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TextDoc(201 tokens; \"RT @FrankGirardot: If @Budweiser is now called ...\"),\n",
       " TextDoc(149 tokens; \"RT @KevinGarifo: It's #Friday, so must be time ...\"),\n",
       " TextDoc(49 tokens; \"I need to get out of my own head before it suff...\"),\n",
       " TextDoc(61 tokens; \"@budlight How can I win the @TheChainsmokers ti...\"),\n",
       " TextDoc(44 tokens; \"#MillerLite / #BudLight #hats for sale on #Ebay...\")]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TextCorpus(745 docs; 159436 tokens)\n"
     ]
    }
   ],
   "source": [
    "print corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc_term_matrix, id2term = corpus.as_doc_term_matrix((doc.as_terms_list(words=True, ngrams=False, named_entities=True)for doc in corpus),weighting='tfidf', normalize=True, smooth_idf=True, min_df=2, max_df=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<745x2927 sparse matrix of type '<type 'numpy.float64'>'\n",
      "\twith 19252 stored elements in Compressed Sparse Row format>\n"
     ]
    }
   ],
   "source": [
    "print(repr(doc_term_matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the tfidf matrix has been, built, we can do topic modeling to see if we can gain any insights on our groups of tweeters.  At this point we can choose between Latent Semantic Analysis (LSA), Latent Dirichlet Allocation (LDA), and Non-negative Matrix Factorization (NMF).  Here is the breakdown for why I will use LSA:\n",
    "\n",
    "- NMF is generally suited for non text data such as pixels, and NMF can return incoherent topics\n",
    "- LDA provides a topical distribution for each topic and term and often is more accurate than LSA\n",
    "- LSA performs Singular Value Decomposition (SVD), which reduces the dimensionality of our already extremely large matrix.  LSA will also execute more quickly which is again important for the end user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = textacy.tm.TopicModel('lsa', n_topics=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.fit(doc_term_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc_topic_matrix = model.transform(doc_term_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(745, 10)\n"
     ]
    }
   ],
   "source": [
    "print(doc_topic_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('topic', 0, ':', u'budlight   tonight   $   beer   bud   @budlight   budweiser   &amp   3   5')\n",
      "('topic', 1, ':', u'nba   detroit   pistons   sport   banner   home   bar   hang   bud   2-sid')\n",
      "('topic', 2, ':', u'$   3   &amp   2   @budlight   5   nba   bar   12:30pm   3.50')\n",
      "('topic', 3, ':', u'@budlight   today   cmafest   cover   princetribute   realmenofgenius   singer   purplerain   rocknroll   accoustic')\n",
      "('topic', 4, ':', u'tonight   today   cmafest   boston   nba   baby   rice   macndcheese   rib   cornonthecob')\n",
      "('topic', 5, ':', u'today   drink   boston   12:30pm   sundayfunday   # boston   event   friyay   pizza   budlight')\n",
      "('topic', 6, ':', u'cmafest   friyay   nashville   guinness   nyc   stellaartoris   setlife   hoegaarden   onset   profoto')\n",
      "('topic', 7, ':', u'cmafest   nashville   kate   today   beer   like   cma   @countrymusic   @995wkdq   wkdq99.5')\n",
      "('topic', 8, ':', u\"budweiser   america   nats   drink   usmnt   usa   baseball   '   dc   gameday\")\n",
      "('topic', 9, ':', u'legend   rocknroll   singer   princetribute   accoustic   realmenofgenius   purplerain   cover   cool   budlight')\n"
     ]
    }
   ],
   "source": [
    "for topic_idx, top_terms in model.top_topic_terms(id2term, top_n=10):\n",
    "    print('topic', topic_idx, ':', '   '.join(top_terms))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the above we used 10 topics so that we can show what is happening in a human readable format, but for the next steps we will want to use more topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(745, 100)\n"
     ]
    }
   ],
   "source": [
    "model = textacy.tm.TopicModel('lsa', n_topics=100)\n",
    "model.fit(doc_term_matrix)\n",
    "doc_topic_matrix = model.transform(doc_term_matrix)\n",
    "print(doc_topic_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our reduced matrix, we can do some clustering to see what hashtags are related\n",
    "\n",
    "Because we don't want the user to have to choose the number of clusters, we have three clustering options from Sklearn that don't require a n_clusters paramter:\n",
    "- DBSCAN generally performs better with more data\n",
    "- MeanShift might perform well but is too computationally intensive\n",
    "- Affinity Prograpagation is the most realistic option here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.99 s, sys: 52.5 ms, total: 2.05 s\n",
      "Wall time: 1.98 s\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import AffinityPropagation\n",
    "ap = AffinityPropagation()\n",
    "%time ap.fit(doc_topic_matrix)\n",
    "clusters = ap.labels_.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the list of clusters, let's connect them with our hashtags to see what hashtags are being grouped together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hashtags</th>\n",
       "      <th>corpus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Canada</td>\n",
       "      <td>RT @FrankGirardot: If @Budweiser is now called...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Friday</td>\n",
       "      <td>RT @KevinGarifo: It's #Friday, so must be time...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>justsayin</td>\n",
       "      <td>I need to get out of my own head before it suf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dance</td>\n",
       "      <td>@budlight How can I win the @TheChainsmokers t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hats</td>\n",
       "      <td>#MillerLite / #BudLight #hats for sale on #Eba...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    hashtags                                             corpus\n",
       "0     Canada  RT @FrankGirardot: If @Budweiser is now called...\n",
       "1     Friday  RT @KevinGarifo: It's #Friday, so must be time...\n",
       "2  justsayin  I need to get out of my own head before it suf...\n",
       "3      dance  @budlight How can I win the @TheChainsmokers t...\n",
       "4       hats  #MillerLite / #BudLight #hats for sale on #Eba..."
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['clusters'] = clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hashtags</th>\n",
       "      <th>corpus</th>\n",
       "      <th>clusters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Canada</td>\n",
       "      <td>RT @FrankGirardot: If @Budweiser is now called...</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Friday</td>\n",
       "      <td>RT @KevinGarifo: It's #Friday, so must be time...</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>justsayin</td>\n",
       "      <td>I need to get out of my own head before it suf...</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dance</td>\n",
       "      <td>@budlight How can I win the @TheChainsmokers t...</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hats</td>\n",
       "      <td>#MillerLite / #BudLight #hats for sale on #Eba...</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    hashtags                                             corpus  clusters\n",
       "0     Canada  RT @FrankGirardot: If @Budweiser is now called...        59\n",
       "1     Friday  RT @KevinGarifo: It's #Friday, so must be time...        48\n",
       "2  justsayin  I need to get out of my own head before it suf...        83\n",
       "3      dance  @budlight How can I win the @TheChainsmokers t...        83\n",
       "4       hats  #MillerLite / #BudLight #hats for sale on #Eba...        76"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_ten = df['clusters'].value_counts()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83    118\n",
       "84     53\n",
       "34     20\n",
       "78     17\n",
       "10     16\n",
       "56     15\n",
       "28     14\n",
       "58     14\n",
       "71     13\n",
       "18     12\n",
       "Name: clusters, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_ten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([83, 84, 34, 78, 10, 56, 28, 58, 71, 18], dtype='int64')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_ten.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster #: 83 Hashtags: 118\n",
      "[u'justsayin' u'dance' u'jamaica' u'WestVsTheRest' u'WalmartSucks'\n",
      " u'WhatYeeKnow' u'securityissue' u'wetrepublic' u'sexyrexyshultz'\n",
      " u'Raiders']\n",
      "Cluster #: 84 Hashtags: 53\n",
      "[u'JH34' u'HappySunday' u'ribchi' u'HouseBoatofReps' u'youshouldbehere'\n",
      " u'lovesummer' u'toxic' u'drinkspecial' u'SportsStars' u'thestruts']\n",
      "Cluster #: 34 Hashtags: 20\n",
      "[u'PALS' u'YellowPissWater' u'weddingday' u'lathtr' u'hff16' u'boring'\n",
      " u'meetatthebar' u'marathontraining' u'soooohootttttt' u'QZOO']\n",
      "Cluster #: 78 Hashtags: 17\n",
      "[u'beberexha' u'yummy' u'PlayStation' u'wingnight' u'snoopdogg' u'LIT'\n",
      " u'oldschool' u'Spurs' u'budlightmusic' u'StanleyCup']\n",
      "Cluster #: 10 Hashtags: 16\n",
      "[u'FreebeeExperience' u'BudLightLime' u'budweiser' u'enjoyacoldone' u'DJ'\n",
      " u'COPA' u'copaam\\xe9ricacentenario' u'minnows4life' u'PROGRESSIVEVIRUS'\n",
      " u'craftbeer']\n",
      "Cluster #: 56 Hashtags: 15\n",
      "[u'40oz' u'MaltLiquor' u'PuttPuttforPaws' u'lime' u'happytaco' u'lakelife'\n",
      " u'fitness' u'Merica' u'MonkeyBombs' u'merica']\n",
      "Cluster #: 28 Hashtags: 14\n",
      "[u'gfquotes' u'thisbudsforyou' u'luminato' u'throwbackthursday' u'party'\n",
      " u'allincle' u'TOC2016' u'strawBERrita' u'eliyoungband' u'Superbowl']\n",
      "Cluster #: 58 Hashtags: 14\n",
      "[u'InsideAmy' u'boycottbudlight' u'mexican' u'virtualhighfive' u'LGBTQ'\n",
      " u'toasttomarriage' u'buildawallaroundyou' u'LGBT' u'ICYMI' u'BlindHatred']\n",
      "Cluster #: 71 Hashtags: 13\n",
      "[u'Ebay' u'eBay' u'Collectible' u'Discount' u'Sales' u'Basketball'\n",
      " u'Auction' u'Buzz' u'NBA' u'Detroit']\n",
      "Cluster #: 18 Hashtags: 12\n",
      "[u'SummerFridays' u'karaoke' u'DjThr33' u'TGIF' u'PawsOnThePatio' u'Nachos'\n",
      " u'jagermeister' u'PubLife' u'tgif' u'ClubTub']\n"
     ]
    }
   ],
   "source": [
    "for i in top_ten.index:\n",
    "    print 'Cluster #: ' + str(i) + ' Hashtags: ' + str(top_ten.ix[i])\n",
    "    print df['hashtags'][df.clusters == i].values[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try this all again with DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 186 ms, sys: 6.73 ms, total: 193 ms\n",
      "Wall time: 192 ms\n",
      "Cluster #: 0 Hashtags: 276\n",
      "[u'Canada' u'justsayin' u'dance' u'BuildABar' u'Pride' u'JH34' u'yummy'\n",
      " u'ShakersSaturday' u'pride' u'Cubs']\n",
      "Cluster #: -1 Hashtags: 274\n",
      "[u'Friday' u'hats' u'cooliecup' u'whatsmakeup' u'Pride2016' u'beberexha'\n",
      " u'swag' u'jamaica' u'rogerscentre' u'Thankyou']\n",
      "Cluster #: 13 Hashtags: 13\n",
      "[u'Ebay' u'eBay' u'Collectible' u'Discount' u'Sales' u'Basketball'\n",
      " u'Auction' u'Buzz' u'NBA' u'Detroit']\n",
      "Cluster #: 3 Hashtags: 8\n",
      "[u'PrinceTribute' u'PurpleRain' u'rocknroll' u'RealMenOfGenius' u'Legend'\n",
      " u'Cover' u'accoustic' u'singer']\n",
      "Cluster #: 5 Hashtags: 8\n",
      "[u'ThirstyThursday' u'jimmygreens' u'fireball' u'Fireball' u'special'\n",
      " u'thirstythursday' u'booze' u'sportsbar']\n",
      "Cluster #: 1 Hashtags: 8\n",
      "[u'SundayFunday' u'sundayfunday' u'atl' u'Pizza' u'pizza' u'ATL' u'Cobb'\n",
      " u'Draft']\n",
      "Cluster #: 29 Hashtags: 7\n",
      "[u'promo' u'Boston' u'boston' u'downtownboston' u'partybus' u'events'\n",
      " u'parade']\n",
      "Cluster #: 26 Hashtags: 7\n",
      "[u'Summer' u'downforwhatever' u'summer' u'surfyogabeer' u'yoga' u'yogi'\n",
      " u'yogagirl']\n",
      "Cluster #: 15 Hashtags: 7\n",
      "[u'marshalltroyphotography' u'profoto' u'nyc' u'onset' u'setlife'\n",
      " u'hoegaarden' u'stellaartoris']\n",
      "Cluster #: 23 Hashtags: 7\n",
      "[u'mangorita' u'countrygirl' u'southernbelle' u'cowgirl' u'lukebryan'\n",
      " u'summerconcert' u'lightsouttour']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "dbs = DBSCAN()\n",
    "%time dbs.fit(doc_topic_matrix)\n",
    "clusters = dbs.labels_.tolist()\n",
    "df['clusters'] = clusters\n",
    "top_ten = df['clusters'].value_counts()[:10]\n",
    "for i in top_ten.index:\n",
    "    print 'Cluster #: ' + str(i) + ' Hashtags: ' + str(top_ten.ix[i])\n",
    "    print df['hashtags'][df.clusters == i].values[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.3 ms, sys: 2.13 ms, total: 13.4 ms\n",
      "Wall time: 12 ms\n",
      "Cluster #: 0 Hashtags: 562\n",
      "[u'Canada' u'Friday' u'justsayin' u'dance' u'hats' u'BuildABar'\n",
      " u'boersandbernsteinshow' u'whatsmakeup' u'Pride2016' u'Pride']\n",
      "Cluster #: -1 Hashtags: 62\n",
      "[u'cooliecup' u'yall' u'jackdaniels' u'jarritos' u'kmelsummerjam' u'twins'\n",
      " u'germanfest' u'finallyfuckinleft' u'ionaz'\n",
      " u'UMustStartDrinkingEarlyAzFuck']\n",
      "Cluster #: 8 Hashtags: 13\n",
      "[u'Ebay' u'eBay' u'Collectible' u'Discount' u'Sales' u'Basketball'\n",
      " u'Auction' u'Buzz' u'NBA' u'Detroit']\n",
      "Cluster #: 16 Hashtags: 12\n",
      "[u'Summer' u'ribfest2016' u'downforwhatever' u'summer' u'InThrees'\n",
      " u'LimeaRita' u'TheLodge' u'RibFest' u'surfyogabeer' u'yoga']\n",
      "Cluster #: 3 Hashtags: 12\n",
      "[u'jamaica' u'Pasadena' u'MexicoVsJamaica' u'Mexico' u'soccer' u'stencil'\n",
      " u'Rosebowl' u'mexico' u'AtlanticCity' u'doac']\n",
      "Cluster #: 1 Hashtags: 12\n",
      "[u'SundayFunday' u'WhatYeeKnow' u'sundayfunday' u'atl' u'Pizza' u'pizza'\n",
      " u'vezzo' u'ATL' u'LikeATank' u'Cobb']\n",
      "Cluster #: 15 Hashtags: 8\n",
      "[u'shihtzu' u'heretoparty' u'dogslife' u'WorkHardPlayHard' u'bearhappy'\n",
      " u'TurntUp' u'badgers' u'100MileHouse']\n",
      "Cluster #: 13 Hashtags: 8\n",
      "[u'killthelights' u'mangorita' u'countrygirl' u'southernbelle' u'cowgirl'\n",
      " u'lukebryan' u'summerconcert' u'lightsouttour']\n",
      "Cluster #: 2 Hashtags: 8\n",
      "[u'PrinceTribute' u'PurpleRain' u'rocknroll' u'RealMenOfGenius' u'Legend'\n",
      " u'Cover' u'accoustic' u'singer']\n",
      "Cluster #: 9 Hashtags: 6\n",
      "[u'BuffaloWildWings' u'Hotwings' u'Drinksonme' u'wildwings'\n",
      " u'budlightPlatinum' u'budlightplatinum']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "%time dist = 1 - cosine_similarity(doc_topic_matrix)\n",
    "from sklearn.cluster import DBSCAN\n",
    "dbs = DBSCAN(metric='precomputed')\n",
    "%time dbs.fit(dist)\n",
    "clusters = dbs.labels_.tolist()\n",
    "df['clusters'] = clusters\n",
    "top_ten = df['clusters'].value_counts()[:10]\n",
    "for i in top_ten.index:\n",
    "    print 'Cluster #: ' + str(i) + ' Hashtags: ' + str(top_ten.ix[i])\n",
    "    print df['hashtags'][df.clusters == i].values[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And once more with MeanShift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.54 s, sys: 61.4 ms, total: 2.6 s\n",
      "Wall time: 2.78 s\n",
      "Cluster #: 0 Hashtags: 722\n",
      "[u'Canada' u'Friday' u'justsayin' u'dance' u'hats' u'SundayFunday'\n",
      " u'BuildABar' u'boersandbernsteinshow' u'cooliecup' u'whatsmakeup']\n",
      "Cluster #: 3 Hashtags: 5\n",
      "[u'rebelt5' u'teamcanon' u'canonrebel' u'beachmusic' u'northhills']\n",
      "Cluster #: 2 Hashtags: 5\n",
      "[u'GameDay' u'surfing' u'surf' u'sunshine' u'SunGlasses']\n",
      "Cluster #: 1 Hashtags: 5\n",
      "[u'mural' u'onmytravel' u'onmyrte66' u'Springfield' u'illinois']\n",
      "Cluster #: 5 Hashtags: 4\n",
      "[u'twins' u'Brazilian' u'Rio' u'Bitburger']\n",
      "Cluster #: 4 Hashtags: 4\n",
      "[u'fuckedup' u'cansorbottels' u'paperorplastic' u'themen']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import MeanShift\n",
    "ms = MeanShift()\n",
    "%time ms.fit(doc_topic_matrix)\n",
    "clusters = ms.labels_.tolist()\n",
    "df['clusters'] = clusters\n",
    "top_ten = df['clusters'].value_counts()[:10]\n",
    "for i in top_ten.index:\n",
    "    print 'Cluster #: ' + str(i) + ' Hashtags: ' + str(top_ten.ix[i])\n",
    "    print df['hashtags'][df.clusters == i].values[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "- 6/15 Used textacy and setup LSA, but it doesn't seem to return anything actionable, going to try clustering by cosine similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connect tweets, tweeter counts, work on summarizer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "add user ids for direct marketing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
